<!-- 
Generated by LectureQ (Enhanced Comprehensive Version)
Date: 2025-11-12 00:33:46
Model: qwen3:30b
Output Mode: Comprehensive
-->

# Week 9: Static and Dynamic Code Analysis

## Overview
This lecture covers two fundamental approaches to security testing: **Static Code Analysis** (SAST) and **Dynamic Code Analysis** (DAST). We'll explore manual code review techniques, advanced static analysis methods (including symbolic execution and taint analysis), and dynamic analysis techniques (including fuzzing and instrumentation). The lecture emphasizes practical applications of these techniques for identifying vulnerabilities in software systems, with a focus on real-world examples and tooling.

---

## Static Code Analysis

### Manual Code Review

**What it is**: A constructive security review of a fellow developer's code, where the goal is to identify security flaws related to features and design, along with exact root causes.

**Key Approaches**:
- **Pair Programming**: Two developers work together at the same computer. One writes code (driver), while the other reviews each line as it's written. *Advantage*: Immediate feedback. *Disadvantage*: May sacrifice depth for speed.
- **Over-the-Shoulder**: Developer seeks feedback by showing code to a colleague, typically after code is ready. *Advantage*: Higher quality review. *Disadvantage*: Delayed feedback.
- **Over-the-Wall**: Code changes are passed between teams without direct collaboration. *Advantage*: Formal process. *Disadvantage*: Least interactive and often least effective.

**Advantages of Secure Code Review**:
- Ensures more than one person has seen each piece of code
- Forces code authors to articulate and document decisions
- Serves as a code education exercise for junior developers
- Assists with familiarization of the codebase
- Identifies complex (logic-based) bugs

> **Key Insight from Transcript**:  
> *"Manual code review can help identify complex and logic-based bugs because tools are mainly based on common vulnerability patterns, not customized for specific applications. Most logic bugs require domain expertise."*  
> *Instructor Emphasis*: "This is why manual review is crucial for identifying subtle vulnerabilities that automated tools miss."  
> *Student Question*: "How do you handle code review when you're unfamiliar with the language?"  
> *Instructor Response*: "First, learn the key syntaxes of the language. Then, use tools designed for that language. Modern tools can help you quickly understand the code structure."

---

### SAST Methods

**Three Core Approaches**:

| Method | Core Idea | Precision | Scalability | Bug Detection Capability |
|--------|-----------|-----------|-------------|--------------------------|
| Pattern Matching | Match code patterns using predefined rules/regex | Low to Medium | Excellent (very fast) | Limited to known/simple issues |
| Symbolic Execution | Explore execution paths with symbolic inputs | High | Poor (path explosion) | Deep bugs, logic flaws |
| Static Taint Analysis | Track flow of untrusted data to sensitive sinks | High | Medium | Injection, misuse of untrusted data |

**Pattern Matching**:
- Uses lexical analysis to scan source code tokens for predefined dangerous patterns
- Example: Identifying unsafe C library functions like `gets`, `strcpy`, `strlen`
- **Why it matters**: These functions are prone to buffer overflows and other vulnerabilities
- **Practical Example**: 
  ```c
  char buffer[10];
  gets(buffer);  // UNSAFE - vulnerable to buffer overflow
  ```
  *Fix*: Use `fgets(buffer, sizeof(buffer), stdin)` instead

> **Key Insight from Transcript**:  
> *"Pattern matching can lead to false positives. For example, a tool might flag a comment like `if (a == b)` as a potential issue, but it's actually a comment, not code."*  
> *Instructor Emphasis*: "Always verify tool findings—false positives are common with pattern matching."

**Vulnerable C Library Functions**:
- `gets` → Use `fgets` instead
- `strcpy` → Use `strncpy` (more compatible) or `strlcpy` (more safe)
- `strlen` → Use `strnlen` instead (or `memchr` for compatibility)

> **Instructor Insight**: *"Man pages are your friend! Always check the documentation before using a function."*

**Symbolic Execution**:
- **Concept**: Execute program with symbolic values instead of concrete values
- **Goal**: Achieve good path coverage by representing equivalence classes of inputs with first-order logic formulas
- **How it works**:
  1. Assign symbolic values to inputs (e.g., `x = a`, `y = b`)
  2. Track symbolic values through program execution
  3. Generate path constraints (e.g., `a == 2b` and `a > b + 10`)
  4. Solve constraints to find representative inputs that trigger specific paths

**Example**:
```c
void func(int x, int y) {
    int z = 2 * y;
    if (z == x) {
        if (x > y + 10)
            ERROR
    }
}
```

**Path Constraints**:
- Path 1: `a != 2b` (no error)
- Path 2: `a == 2b && a <= b + 10` (no error)
- Path 3: `a == 2b && a > b + 10` (triggers error)

> **Key Insight from Transcript**:  
> *"Symbolic execution narrows down the scope of values you need to test. Instead of trying infinite integer combinations, it identifies specific regions of input space that could trigger vulnerabilities."*  
> *Instructor Emphasis*: "This is why symbolic execution is more efficient than random testing for security vulnerabilities."

**Static Taint Analysis**:
- **Purpose**: Track propagation of data through a program
- **Components**:
  - **Sources**: Taint seeds (untrusted input, sensitive data)
  - **Propagation**: Taint policy (variables based on tainted data are also tainted)
  - **Sinks**: Taint checking/asserts (special calls where tainted data causes issues)

**Real-World Example** (SQL Injection):
```python
import sqlite3
from flask import request

def get_user_data():
    username = request.args.get("user")  # Tainted source: user input
    query = f"SELECT * FROM users WHERE name = '{username}'"  # Dangerous if unsanitized
    conn = sqlite3.connect("mydb.db")
    cursor = conn.cursor()
    cursor.execute(query)  # Sink: SQL query execution
```

**Fix**:
```python
def get_user_data_safe():
    username = request.args.get("user")
    query = "SELECT * FROM users WHERE name = ?"
    cursor.execute(query, (username,))  # Parameterized query prevents injection
```

> **Key Insight from Transcript**:  
> *"Taint analysis can be done in two ways: starting from the source (user input) and tracing to the sink (SQL execution), or starting from the sink and tracing back to the source."*  
> *Instructor Emphasis*: "This is crucial for identifying vulnerabilities like SQL injection, where tainted input reaches a critical operation."

---

### Limitations of Static Analysis

**Undecidability of Static Analysis**:
- Determining whether a program satisfies a property is theoretically undecidable (reduces to the halting problem)
- **Why it matters**: No perfect static analysis tool can exist

**Soundness and Completeness**:
- **Soundness**: Everything the analyzer finds is a real bug (but some bugs may be missed)
- **Completeness**: The analyzer finds every bug (but may have false positives)
- **Reality**: Most static analyzers are neither sound nor complete

> **Key Insight from Transcript**:  
> *"Static analysis tools can't be perfect because they'd have to solve the halting problem. But they're still useful—they help identify potential issues before developers have to manually review thousands of lines of code."*  
> *Instructor Emphasis*: "Think of static analysis as a filter to reduce the number of issues you need to manually inspect."

---

## Dynamic Code Analysis

### Core Principles

**Conceptual State Machine**:
- Represents the intended operation of a program
- States: Different sets of inputs/outputs the program can produce
- Transitions: Changes between states (e.g., when conditions change)

**State Classification**:
- **Intended States**: States the program is designed to reach
- **Transition States**: States between intended states
- **Unintended States**: States representing vulnerabilities
- **Unreachable States**: States the program will never reach

> **Key Insight from Transcript**:  
> *"Vulnerabilities live in unintended states. Exploitation is making the program do 'interesting' transitions in the unintended state space."*  
> *Instructor Emphasis*: "This is why we need to identify transition states—those are the paths to unintended states."

**Bug Classification**:
- **Design Issue**: Conceptual state machine doesn't meet requirements (e.g., hardcoded admin password)
- **Functionality Bug**: Bad transitions between valid states (e.g., broken save button)
- **Implementation Bug**: New states not represented in design (e.g., lack of length checks causing stack corruption)
- **Hardware Fault**: Hardware glitch causing unintended state (e.g., cosmic ray bit flip)
- **Transmission Error**: Data corruption during transfer (e.g., unencrypted data in transit)

> **Instructor Insight**:  
> *"Security by design is crucial. If you don't consider potential vulnerabilities early, you'll have to fix them later at much higher cost."*

---

### Dynamic Analysis Techniques

**DAST Methods**:
- **Fault Injection**: Inputting unsafe, unsanitized values
- **Memory Reference Errors**: Uninitialized memory, segment faults, memory leaks
- **Coordination Problems**: Race conditions in concurrent programs
- **Security of Web Applications**: Tainted values

**Instrumentation**:
- **Observation**: Hardware monitoring, PC sampling, breakpoints
- **Instrumentation**: Code added to original program (ideally doesn't affect semantics)
- **How much to generate**: Everything, just necessary facts, or less than necessary

**Dynamic Analysis Tools**:

| Tool | Purpose | Key Features | Speed Impact |
|------|---------|--------------|--------------|
| **Valgrind** | Memory error detection | Detects memory leaks, use-after-free, invalid reads/writes | 10-20x slowdown |
| **ASAN** | Address Sanitizer (Google) | Detects heap buffer overflows, use-after-free, stack overflows | ~2x slowdown |
| **TSan** | ThreadSanitizer | Detects data races in concurrent programs | 5-10x memory, 5-15x slowdown |

**How ASAN Works**:
- Poisons memory around malloc-ed regions (red zones)
- Places freed memory in quarantine and poisons it
- Detects memory errors by checking poisoned memory

> **Key Insight from Transcript**:  
> *"ASAN is fast compared to Valgrind—it's only about 2x slower, whereas Valgrind can be 10-20x slower. This makes ASAN practical for regular testing."*  
> *Instructor Emphasis*: "For memory safety, ASAN is the tool of choice for most developers."

---

### Fuzzing

**Concept**: Find bugs by feeding programs random, corrupted, or unexpected data.

**Why it works**: Random inputs explore large parts of the state space. Any crash is a bug (though not all bugs are exploitable).

**Fuzzing Strategies**:

| Strategy | Input Source | Input Validity | Learning from Execution | Complexity |
|----------|----------------|----------------|--------------------------|------------|
| **Mutation-Based** | Existing files | Often low | No | Simple |
| **Generation-Based** | Input specification | High (if spec is good) | No | Complex |
| **Coverage-Guided** | Any (mutation/generation) | Medium to High | Yes | Moderate |

**Mutation-Based Fuzzing**:
1. Collect corpus of inputs exploring many states
2. Randomly perturb inputs (bit flips, integer increments)
3. Run program on inputs and check for crashes
4. Repeat

**Real-World Example**: Charlie Miller's PDF viewer fuzzing in 2010:
```python
numwrites = random.randrange(math.ceil((float(len(buf)) / FuzzFactor))) + 1
for j in range(numwrites):
    rbyte = random.randrange(256)
    rn = random.randrange(len(buf))
    buf[rn] = "%c" % (rbyte)
```
*Result*: Found 64 exploitable crashes using simple mutation.

> **Key Insight from Transcript**:  
> *"Dumb fuzzing is often way more successful than it has any right to be. It's simple to set up and can find critical vulnerabilities."*  
> *Instructor Emphasis*: "Don't underestimate the power of simple mutation-based fuzzing—it's often the first step in finding critical vulnerabilities."

**Coverage-Guided Fuzzing**:
- **Core Idea**: Use code coverage as feedback to guide fuzzing
- **Metrics**:
  - Basic block coverage: Has this basic block been run?
  - Edge coverage: Has this branch been taken?
  - Path coverage: Has this specific path been taken?

**American Fuzzy Lop (AFL)**:
1. Compile program with instrumentation to measure coverage
2. Trim test cases to smallest size without changing behavior
3. Create new test cases by mutating files in the queue
4. If new coverage is found, add to queue
5. Repeat

**Example**:
```c
int main(int argc, char *argv[]) {
    char buf[100];
    FILE *fp = fopen(argv[1], "rb");
    if (!fp) return 1;
    fread(buf, 1, sizeof(buf), fp);
    if (buf[0] == 'A')           // Path 1
      if (buf[1] == 'B')         // Path 2
        if (buf[2] == 'C')       // Path 3
          if (buf[3] == 'D')     // Path 4
            if (buf[4] == 'X')   // Path 5 → CRASH!
              crash();
    return 0;
}
```

**AFL Execution Process**:
| Step | Input | Path Taken | New Coverage? | Action |
|------|-------|------------|---------------|--------|
| 1 | "" | Start only | Yes | Keep |
| 2 | "A" | Path 1 | Yes | Keep |
| 3 | "Z" | Invalid | No | Discard |
| 4 | "AB" | Path 1-2 | Yes | Keep |
| 5 | "AC" | Path 1 | No | Discard |
| 6 | "ABC" | Path 1-2-3 | Yes | Keep |
| 7 | "ABZ" | Path 1-2 | No | Discard |
| 8 | "ABCD" | Path 1-2-3-4 | Yes | Keep |
| 9 | "ABCDX" | Path 1-2-3-4-5 | Yes | CRASH → SAVE! |

> **Key Insight from Transcript**:  
> *"Coverage-guided fuzzing is wildly successful because it combines well with other strategies and is very good at finding new program states."*  
> *Instructor Emphasis*: "This is why AFL is the most popular fuzzing tool today—it's simple to use and highly effective."

---

## SAST vs DAST

| Factor | SAST | DAST |
|--------|------|------|
| **Coverage** | Full codebase | Limited to executed paths |
| **SDLC Integration** | Early (during development) | Later (after code is built) |
| **Easy to Setup** | Yes | No |
| **Code Visibility** | Full source code | Limited (black box) |
| **Remediation Advice** | Specific line numbers | General guidance |
| **False Positives** | Medium to High | Low |
| **Verify Exploitability** | No | Yes |

> **Key Insight from Transcript**:  
> *"Dynamic analysis leaves feasible paths unexplored, so it may conclude a property holds when it really doesn't. Static analysis explores infeasible paths, so it may conclude a property doesn't hold when it really does."*  
> *Instructor Emphasis*: "This is why you need both—static for early detection and dynamic for verification."

---

## Integrating Security Testing into SDLC

**Best Practices**:
- **Unit Tests**: Check each piece of code behaves as expected in isolation (cover all code, including error handling)
- **Regression Tests**: Check old bugs haven't been reintroduced (attackers will run these for you if you don't)
- **Integration Tests**: Check modules work together as expected

**DAST Integration**:
- Begin testing early (sufficient time for DAST)
- Schedule regular testing times
- Pick tools that make instrumentation convenient

> **Instructor Insight**:  
> *"Most exploitable bugs would be eliminated with basic unit tests. Don't wait until the end to test—integrate security testing throughout the development lifecycle."*

---

## General Secure Programming Tips

1. **Use memory-safe languages** where possible (Go, Rust, Swift)
2. **Understand and document your threat model** early in design
3. **Design APIs so the easiest way to use them is the safe way**
4. **Treat all external input adversarially**, even if you trust the sender
5. **Use a clean, consistent style** throughout the codebase

> **Key Insight from Transcript**:  
> *"Security is a process, not a product. It's about making security a natural part of your development workflow, not an afterthought."*  
> *Instructor Emphasis*: "This is why we're teaching you these techniques—you need to integrate them into your daily workflow."

---

## Summary: Key Takeaways

1. **Static Analysis** (SAST) examines code without execution:
   - **Pattern Matching**: Fast but low precision (e.g., `gets` → `fgets`)
   - **Symbolic Execution**: High precision but suffers from path explosion
   - **Taint Analysis**: Tracks data flow from source to sink (e.g., SQL injection)

2. **Dynamic Analysis** (DAST) examines program behavior during execution:
   - **Fault Injection**: Inputting unsafe values
   - **Memory Error Detection**: Tools like Valgrind and ASAN
   - **Fuzzing**: Random input generation to find crashes (mutation, generation, coverage-guided)

3. **Critical Limitations**:
   - Static analysis can't be perfect (undecidable problem)
   - Both SAST and DAST have false positives/negatives
   - **Use both**: SAST for early detection, DAST for verification

4. **Practical Integration**:
   - Include security testing in all phases of SDLC
   - Use tools like AFL for fuzzing, ASAN for memory safety
   - Combine manual review with automated tools

---

## Study Questions

1. What are the three main approaches to security testing, and how do they differ?
2. Explain the difference between soundness and completeness in static analysis.
3. Why is manual code review still important despite automated tools?
4. How does symbolic execution differ from traditional testing?
5. What is the key insight behind taint analysis, and how does it help identify SQL injection vulnerabilities?
6. Why can't static analysis tools be perfect, and what does this mean for their practical use?
7. Describe the three main fuzzing strategies and their advantages/disadvantages.
8. How does coverage-guided fuzzing (like AFL) work, and why is it more effective than random fuzzing?
9. What are the main limitations of static analysis that make dynamic analysis necessary?
10. Why is it important to integrate security testing throughout the SDLC rather than as a final step?
11. What are the key differences between SAST and DAST, and when would you use each?
12. How does ASAN work to detect memory errors, and why is it faster than Valgrind?
13. What is the significance of "intended states" and "unintended states" in dynamic analysis?
14. Why is it important to treat all external input adversarially, even if you trust the sender?
15. How does the concept of "state machines" help in understanding software vulnerabilities?
16. What are the three main types of bugs (design, functionality, implementation) and how do they manifest?
17. Why is it important to have a threat model early in the design process?
18. How does the example of SQL injection demonstrate the importance of taint analysis?
19. What are the key advantages of using a parameterized query over string concatenation for SQL queries?
20. How would you explain the concept of "false positives" in static analysis to a non-technical stakeholder?