<!-- 
Generated by LectureQ
Date: 2025-08-08 02:27:17
Model: qwen3:30b
-->

<think>
We are creating comprehensive study notes for Chapter 27: Safety Critical Systems (Week 1 Introduction) from the textbook.

The chapter content is provided in 9 slides (from Slide 1 to Slide 9). We'll structure the notes to follow the chapter's flow.

Key sections we see in the slides:
- Introduction to safety-critical systems and their relevance to security
- Safety engineering methodologies (27.3.1)
- Hazard analysis (27.3.2)
- Fault trees and threat trees (27.3.3)
- Failure modes and effects analysis (FMEA) (27.3.4)
- Threat modeling (27.3.5)
- Quantifying risks (27.3.6)

We'll also note the case studies and examples provided.

Let's break down the notes accordingly.

Important: We must extract all key information, define terms, and structure for exam preparation.

We'll start with:
- Chapter title and a brief overview (since learning objectives aren't explicitly stated, we'll infer from the content)
- Then, we'll go through each section in order.

We note that the chapter is from "Anderson c27.tex", and the page numbers are provided (e.g., Page 969, 970, etc.), but we won't include page numbers in the notes unless they are critical for reference (which they aren't for study notes).

We'll create:
- Hierarchical headers
- Bold key terms
- Bullet points for lists
- Tables for comparisons (if any)
- Blockquotes for key insights
- Summary boxes for complex concepts
- A comprehensive summary at the end
- Review questions (if any are mentioned; we don't see any in the provided text, so we'll skip or create a few based on the content)

Let's begin.

Note: The chapter is titled "27.3 Lessons from safety-critical systems", but the overall chapter is "Chapter 27: Secure Systems Development". We'll use the chapter title as given.

We'll structure the notes as:

# Chapter 27: Secure Systems Development - Safety Critical Systems (Week 1 Introduction)

## Learning Objectives
* Understand the principles of safety-critical systems and their relevance to security engineering.
* Learn key methodologies from safety engineering (hazard analysis, fault trees, FMEA) and how they apply to security.
* Recognize the importance of integrating safety and security from the design phase.
* Analyze case studies of failures in safety-critical systems and their lessons for security.

## Key Concepts and Definitions

### Safety-Critical Systems
- Systems in which a certain class of failure must be avoided at all costs.
- Examples: flight controls, automatic braking systems.
- **Criticality classes**: safety-critical, business-critical, security-critical, critical to the environment.

### Safety Case
- A structured argument that provides evidence that a system is safe to operate.
- Typically includes: hazard analysis, safety functional and integrity requirements, test results (component and system level).
- Required for regulatory compliance and to demonstrate due care in case of failure.

### Hazard Elimination
- The process of designing hazards out of a system completely.
- Example: In motor reversing circuits, swapping battery and motor to avoid shorting the battery (Slide 3).

### Fault Tree Analysis (FTA)
- A top-down method to identify failures that could cause accidents.
- Root: undesired behavior; nodes: possible causes.
- In security, called **Threat Tree**.

### Failure Modes and Effects Analysis (FMEA)
- A bottom-up method to trace the consequences of component failures to system effects.
- Pioneered by NASA.
- Focuses on failure modes, their effects, and severity.

### Threat Modeling
- Systematic process to identify, quantify, and address threats to a system.
- Microsoft's approach: meet-in-the-middle (combines top-down and bottom-up).

### Quantifying Risks
- Using statistical methods to measure failure rates, error rates, etc.
- **Human Error**: Depends on task familiarity, complexity, pressure, and cues (e.g., error rate 1 in 100,000 for simple, frequent tasks).

### Safety Usability
- Designing systems so that in emergencies, operators can make safe decisions.
- Principles: default to safe state, keep information and controls simple and intuitive.

### Fault Masking
- When a system continues to function after a fault (e.g., majority voting), but the safety margin is reduced and the fault may not be visible to operators.

## Detailed Study Notes

### 27.3 Lessons from Safety-Critical Systems

#### Introduction
- Safety-critical systems are those where a specific class of failure must be avoided (e.g., flight controls, automatic braking).
- The safety engineering literature offers methodologies that can be adapted to security engineering.
- **Key Insight**: Safety deals with random failures; security deals with hostile adversaries who can cause failures at the worst time.

> "Safety engineering is not just about correct operation, but about correct failure too." (Slide 3)

#### 27.3.1 Safety Engineering Methodologies
- **Systematic approach**: from safety analysis to specification to product, building safety in from the start (not retrofitting).
- **Procedure**:
  1. Identify hazards and assess risks.
  2. Decide on strategy (avoidance, constraint, redundancy).
  3. Trace hazards to critical hardware/software components.
  4. Identify critical operator procedures (considering psychology and operations research).
  5. Set safety functional requirements (what safety mechanisms must do).
  6. Set safety integrity requirements (likelihood of safety function being performed satisfactorily).
  7. Decide on test plan (integrated part of safety case).
- **Standards**:
  - ISO 61508: Basic safety framework for programmable electronics (e.g., chemical plant controls).
  - ISO 26262: For road vehicles (extended from ISO 61508).

> **Safety Case**: Evidence for due care; includes hazard analysis, requirements, test results. Must be maintained (not sidelined after approval).

#### 27.3.2 Hazard Analysis
- **Goal**: Eliminate hazards where possible.
- **Example: Motor Reversing Circuit** (Figure 27.2):
  - Original design (a): Double-pole double-throw switch. If one pole fails, battery shorts → fire.
  - Modified design (b): Swap battery and motor. Switch failure only shorts motor (not battery) → safer.
- **Application to Security**:
  - Minimize trusted computing base (TCB). Example: SWIFT design (Section 12.3.2) – keys exchanged directly between banks, so SWIFT didn't need to be trusted.
- **Application to Privacy**:
  - Central database for contact tracing → privacy hazard.
  - Alternative: Keep Bluetooth contact history on mobile phones → better privacy.

> **Key Principle**: "Minimizing the trusted computing base is an exercise in hazard elimination."

#### 27.3.3 Fault Trees and Threat Trees
- **Fault Tree Analysis (FTA)**:
  - Top-down: Root = undesired behavior; nodes = causes.
  - Used in safety engineering.
- **Threat Tree** (in security):
  - Same structure but for security threats.
  - Example: Fraud from ATMs (Figure 27.3) – shows attack paths (shoulder surfing, cryptanalysis, etc.).
- **DoD Requirement**: Threat trees are used and can be highly classified.
- **Benefits**:
  - Visualize commonality between attack paths.
  - Reason about disrupting most attacks with least effort.
  - Can include countermeasures (with sub-branches).

> **Insight**: "A threat tree can amount to an attack manual for the system."

#### 27.3.4 Failure Modes and Effects Analysis (FMEA)
- **Bottom-up approach** (tracing from component failure to system effect).
- **Pioneered by NASA** for aerospace.
- **Process**:
  - Identify each component's failure modes.
  - Trace effects up to mission impact.
  - Prioritize based on severity and likelihood.
- **Case Study: Challenger Disaster (1986)**:
  - O-rings in booster rockets: known risk (damage on previous flights), contractor knew cold increased risk.
  - Concerns didn't reach top management.
  - O-ring became brittle in cold → failed → shuttle lost.
  - **Lesson**: Failures are not just technical but involve organizational behavior (e.g., communication across institutional boundaries).

> **Key Insight**: "When protection mechanisms cross institutional boundaries, you need to think of the law and economics as well as just the engineering."

#### 27.3.5 Threat Modeling
- **Challenge**: FTA and FMEA are hard to automate and can be disrupted by subtle changes.
- **Microsoft's Approach (2003)**:
  - **Meet-in-the-middle**: Combines top-down and bottom-up.
  - Steps:
    1. List assets to protect (e.g., transaction ability, confidential data).
    2. List assets available to attacker (e.g., ability to subscribe, manipulate inputs).
    3. Trace attack paths through the system.
    4. Identify trust levels, barriers, and techniques (spoofing, tampering, etc.).
  - **Use in SDLC**: Architecture reviews, code reviews, penetration tests.
- **Managing Data**:
  - **Matrix Approach**: Hazards vs. safety mechanisms; require at least two independent mechanisms per serious hazard.
  - **System Theoretic Process Analysis (STPA)**: Starts with hazards and designs controls top-down; helps with interacting control loops.

> **Key Insight**: "Modern large-scale systems are typically designed to limit damage if a single component is compromised."

#### 27.3.6 Quantifying Risks
- **Failure Rate Measurement**:
  - Component failure rates: statistical.
  - Software bugs: tracked by techniques (to be discussed in next chapter).
  - Operator error: depends on task familiarity, complexity, pressure, cues.
- **Error Rate Examples**:
  - Simple, frequent task: 1 in 100,000 operations.
  - Complex, first-time task under pressure: high error probability (e.g., Three Mile Island, Chernobyl).
- **Safety Usability Principles**:
  - **Default to safe state**: e.g., damp nuclear reaction, return aircraft to level flight, stop autonomous vehicle.
  - **Keep information and controls simple and intuitive**.
    - *Aviation Example*: Old planes had 6 critical instruments in center; new planes have 50 alarms → pilots overwhelmed (USS McCain collision, 2017).
- **Human Factors**:
  - Errors rare in frequent tasks (with skill); more likely under stress and surprise.
  - **Security Usability**: Present risks in line with common mental models; design for stress-induced reactions.

> **Case Study: Boeing 737Max**:
  - **Failure**: Flight control software relied on single angle-of-attack sensor (instead of two).
  - **Cause**: Engineers under time pressure assumed pilots could cope with failure (skipped FMEA).
  - **Result**: Two crashes due to faulty sensor → plane dived.
  - **Lesson**: "Satan's computer" (security engineering) is harder to test than "Murphy's computer" (safety engineering).

> **Key Insight**: "A safety engineer will certify a critical flight-control system with an MTBF of 10^9 hours; a security engineer has to worry whether an adversary can force the preconditions for that one-in-a-billion failure and crash the plane on demand."

#### 27.3.7 Integration of Safety and Security
- **Critical Difference**:
  - Safety: random failures.
  - Security: hostile adversary (can cause failures at worst time).
- **Lesson**: "Safety has to be built in as a system is developed, rather than retrofitted." (Same for security.)
- **Modern Shift**: Move from project-based to agile/DevSecOps to embed security in product evolution.

> **Key Insight**: "The main difference between safety and security is in the failure model."

## Summary of Key Methodologies

| Methodology          | Approach      | Key Focus                          | Security Application          |
|----------------------|---------------|------------------------------------|-------------------------------|
| **Fault Tree (FTA)** | Top-down      | Causes of undesired behavior       | Threat Tree for security threats |
| **FMEA**             | Bottom-up     | Component failure effects          | Threat modeling with insider risks |
| **Threat Modeling**  | Meet-in-middle| Asset-based attack paths           | Microsoft's approach          |
| **Safety Case**      | Integrated    | Evidence for due care              | Security case for compliance  |

## Summary Box: Safety vs. Security Engineering

- **Safety Engineering**:
  - Focus: Avoiding random failures (e.g., component failure, human error).
  - Certification: Based on statistical failure rates (e.g., MTBF 10^9 hours).
  - Example: Flight control system.

- **Security Engineering**:
  - Focus: Defending against hostile adversaries (who can cause failures at worst time).
  - Certification: Must consider if adversary can force failure conditions.
  - Example: "Programming Satan's computer" – designing for maliciously wrong answers at worst moment.

> **Core Insight**: "Security engineering is hard because Satan's computer is harder to test."

## Comprehensive Summary

Safety-critical systems engineering provides valuable methodologies for security engineering. Key lessons include:
1. **Build safety/security in from the start** (not retrofit).
2. **Hazard elimination** (e.g., minimizing TCB) is a powerful principle.
3. **Systematic analysis** (FTA, FMEA, threat trees) helps identify and mitigate risks.
4. **Quantify risks** using statistical methods and human factors.
5. **Design for usability in emergencies** (simple controls, default safe state).
6. **Integrate safety/security into the product lifecycle** (DevSecOps).

Critical failures (e.g., Challenger, Boeing 737Max) highlight the importance of:
- Organizational communication (across institutional boundaries).
- Not skipping safety analysis (FMEA) under time pressure.
- Considering both technical and human factors.

Security engineering must account for **hostile adversaries** (unlike safety's random failures), making it more challenging. The goal is to design systems that are resilient even when an adversary actively tries to cause failure.

## Review Questions (Based on Chapter Content)

1. What is the main difference between safety engineering and security engineering in terms of failure models?
2. Describe the process of hazard elimination with an example from the chapter.
3. How does a threat tree differ from a fault tree, and what is its purpose in security engineering?
4. Explain the Challenger disaster in terms of FMEA and organizational behavior.
5. Why is it important to integrate safety cases with mainstream product documentation?
6. What is "fault masking" and why is it a concern in redundant systems?
7. How does the Boeing 737Max case illustrate the challenges of security engineering compared to safety engineering?

---

Now, we'll format the above into a well-structured markdown document.
</think>

# Chapter 27: Secure Systems Development - Safety Critical Systems (Week 1 Introduction)

## Learning Objectives
- Understand principles of safety-critical systems and their relevance to security engineering
- Apply safety engineering methodologies (hazard analysis, FMEA, threat modeling) to security contexts
- Analyze case studies of safety/security failures and their lessons
- Recognize the importance of integrating safety/security from design phase
- Quantify risks using safety engineering techniques
- Design for safety usability in emergency scenarios

---

## Key Concepts & Definitions

| Term | Definition | Relevance to Security |
|------|------------|----------------------|
| **Safety-Critical Systems** | Systems where a specific class of failure must be avoided at all costs | Foundation for security engineering principles |
| **Safety Case** | Structured argument providing evidence that a system is safe to operate | Required for regulatory compliance; includes hazard analysis, requirements, test results |
| **Hazard Elimination** | Designing hazards out of a system completely | Minimizing trusted computing base (e.g., SWIFT design) |
| **Fault Tree Analysis (FTA)** | Top-down method identifying failures causing accidents | Security adaptation: **Threat Tree** |
| **Failure Modes & Effects Analysis (FMEA)** | Bottom-up method tracing component failures to system effects | Security adaptation: Threat modeling with insider risks |
| **Safety Integrity Requirements** | Specification of failure rate likelihood for safety functions | Security equivalent: Threat tolerance levels |
| **Fault Masking** | System continues functioning after failure (e.g., majority voting), but safety margin eroded | Critical in redundant security systems |
| **Satan's Computer** | Security engineering challenge: Programming for maliciously wrong answers at worst moment | Core difference from safety engineering ("Murphy's computer") |

> **Key Insight**: *"Safety engineering is not just about correct operation, but about correct failure too."* (Slide 3)

---

## 27.3 Lessons from Safety-Critical Systems

### Introduction: Safety vs. Security Engineering
- **Safety Engineering**: Deals with *random failures* (e.g., component malfunction, human error)
- **Security Engineering**: Deals with *hostile adversaries* who can cause failures at worst possible time
- **Critical Difference**:  
  > *"A safety engineer will certify a critical flight-control system with an MTBF of 10⁹ hours; a security engineer has to worry whether an adversary can force the preconditions for that one-in-a-billion failure and crash the plane on demand."*

> **Core Principle**: *"Programming Satan's computer is harder to test than programming Murphy's."* (Slide 9)

---

### 27.3.1 Safety Engineering Methodologies
**Systematic Approach (Build-in from Start)**:
1. Identify hazards and assess risks
2. Decide on strategy (avoidance, constraint, redundancy)
3. Trace hazards to critical hardware/software components
4. Identify critical operator procedures (psychology/operations research)
5. Set safety functional requirements (what safety mechanisms must do)
6. Set safety integrity requirements (failure rate likelihood)
7. Develop integrated test plan (part of safety case)

**Key Standards**:
- **ISO 61508**: Basic safety framework for programmable electronics (e.g., chemical plant controls)
- **ISO 26262**: Specialized standard for road vehicles (extended from ISO 61508)

> **Critical Insight**: *"The safety case must be integrated with mainstream product documentation. If separate, it's easily sidelined after approval."* (Slide 9)

---

### 27.3.2 Hazard Analysis
**Goal**: Eliminate hazards where possible before design.

**Motor Reversing Circuit Example** (Figure 27.2):
| Design | Risk | Solution |
|--------|------|----------|
| Double-pole switch (left) | Single pole failure → battery short → fire | Swap battery/motor (right) → switch failure only shorts motor |

**Security Applications**:
- **SWIFT Design (Section 12.3.2)**:  
  *Keys exchanged directly between banks → SWIFT minimized trust → hazard elimination*
- **Privacy Engineering**:  
  *Central database (privacy hazard) vs. Bluetooth contact history on mobiles (better privacy)*

> **Key Principle**: *"Minimizing the trusted computing base is an exercise in hazard elimination."*

---

### 27.3.3 Fault Trees & Threat Trees
**Fault Tree Analysis (FTA)**:
- Top-down approach: Root = undesired behavior → causes
- Used in safety engineering

**Threat Tree (Security Adaptation)**:
- Same structure but for security threats
- Example: ATM fraud threat tree (Figure 27.3) shows attack paths (shoulder surfing, cryptanalysis, etc.)

**Benefits**:
- Visualize commonality between attack paths
- Identify most efficient disruption points
- Can include countermeasures (sub-branches)

> **DoD Requirement**: *"A threat tree can amount to an attack manual for the system."*

---

### 27.3.4 Failure Modes & Effects Analysis (FMEA)
**Bottom-Up Approach**:
- Trace component failures → system effects
- Pioneered by NASA for aerospace

**Process**:
1. Identify component failure modes
2. Trace effects to mission impact
3. Prioritize based on severity/likelihood

**Case Study: Challenger Disaster (1986)**:
- **Failure**: O-rings became brittle in cold → failed
- **Root Cause**:  
  *NASA project manager knew O-ring risk, contractor knew cold increased risk, but concerns didn't cross institutional boundaries*
- **Lesson**: *"Failures are not just technical but involve organizational behavior."*

> **Critical Insight**: *"When protection mechanisms cross institutional boundaries, you need to think of law and economics as well as engineering."*

---

### 27.3.5 Threat Modeling
**Microsoft's Meet-in-the-Middle Approach** (Post-2003 security push):
1. List assets to protect (e.g., transaction ability, confidential data)
2. List attacker assets (e.g., subscription ability, input manipulation)
3. Trace attack paths through system
4. Identify trust levels, barriers, and techniques (spoofing, tampering, etc.)

**Data Management Techniques**:
| Method | Description | Security Application |
|--------|-------------|----------------------|
| **Matrix Approach** | Hazards vs. safety mechanisms; require ≥2 independent mechanisms per hazard | Verify defense-in-depth |
| **STPA** | Top-down hazard → control design | Tease apart interacting control loops |

> **Key Insight**: *"Modern large-scale systems are designed to limit damage if a single component is compromised."*

---

### 27.3.6 Quantifying Risks
**Risk Quantification Methods**:
- Component failure rates: Statistical measurement
- Software bugs: Trackable via metrics
- Operator error: Depends on:
  - Task familiarity & complexity
  - Pressure level
  - Cues to success

**Error Rate Examples**:
| Task Type | Error Rate | Example |
|-----------|------------|---------|
| Simple, frequent | 1 in 100,000 | Routine operation |
| Complex, first-time under pressure | High | Three Mile Island, Chernobyl |

**Safety Usability Principles**:
1. **Default to safe state**:  
   *Damp nuclear reaction → return aircraft to level flight → stop autonomous vehicle*
2. **Keep information/simple controls**:  
   *Old planes: 6 critical instruments centered; New planes: 50 alarms → pilot overload (USS McCain collision, 2017)*

**Boeing 737Max Case Study**:
- **Failure**: Relied on single angle-of-attack sensor (instead of two)
- **Cause**: Engineers skipped FMEA under time pressure → assumed pilots could cope
- **Result**: Two crashes due to faulty sensor → plane dived
- **Lesson**: *"Satan's computer is harder to test than Murphy's computer."*

> **Critical Insight**: *"Security usability isn't just about intuitive interfaces—it must present risks in line with common mental models of threat."*

---

## Summary Box: Safety vs. Security Engineering

| Aspect | Safety Engineering | Security Engineering |
|--------|-------------------|----------------------|
| **Failure Model** | Random failures (component, human) | Hostile adversary causing failures at worst time |
| **Certification Basis** | Statistical failure rates (e.g., MTBF 10⁹ hours) | Adversary's ability to force failure conditions |
| **Core Challenge** | Avoiding rare random failures | Defending against maliciously timed failures |
| **Testing Difficulty** | "Programming Murphy's computer" | "Programming Satan's computer" (harder to test) |
| **Key Principle** | Build safety in from start | Build security in from start |

> **Final Insight**: *"Safety has to be built in as a system is developed, rather than retrofitted. The main difference between safety and security is in the failure model."*

---

## Comprehensive Summary

Safety-critical systems engineering provides essential methodologies for security engineering. Key principles include:
1. **Hazard elimination** (e.g., minimizing trusted computing base) is foundational
2. **Systematic analysis** (FTA, FMEA, threat trees) must be integrated from design phase
3. **Quantify risks** using statistical methods and human factors (error rates depend on task complexity/pressure)
4. **Design for safety usability**: Default to safe state, keep controls simple during emergencies
5. **Integrate safety cases** with mainstream documentation (avoiding post-approval neglect)
6. **Recognize adversary difference**: Security requires defending against *maliciously timed failures* (Satan's computer), making it fundamentally harder than safety engineering

**Critical Lessons from Failures**:
- **Challenger**: Organizational communication failures across institutional boundaries
- **Boeing 737Max**: Skipping FMEA under time pressure → catastrophic failure
- **USS McCain**: UI overload → pilot confusion → collision

**Modern Shift**: Moving from project-based to **DevSecOps** to embed security in product evolution.

---

## Review Questions

1. **Explain the difference between safety engineering and security engineering in terms of failure models.**  
   *Safety deals with random failures (e.g., component malfunction); security deals with hostile adversaries who can cause failures at worst possible time.*

2. **Describe hazard elimination with a security example from the chapter.**  
   *SWIFT design: Keys exchanged directly between banks (not through SWIFT), minimizing trusted computing base and eliminating trust hazards.*

3. **How does a threat tree differ from a fault tree?**  
   *Threat trees are security adaptations of fault trees; they map attack paths (e.g., ATM fraud) instead of technical failures.*

4. **Why was the Challenger disaster a failure of FMEA?**  
   *O-ring risk was known to project manager and contractor, but concerns didn't cross institutional boundaries → FMEA wasn't properly integrated.*

5. **What is "fault masking" and why is it dangerous?**  
   *When a system continues functioning after a fault (e.g., majority voting), but safety margin erodes. Operators may not recognize the degraded state under pressure (e.g., aircraft with one system offline).*

6. **Why is security engineering harder than safety engineering?**  
   *Security requires defending against "Satan's computer" (maliciously wrong answers at worst moment), while safety deals with "Murphy's computer" (random failures). Satan's computer is harder to test.*

7. **How should safety cases be managed to prevent failures like Boeing 737Max?**  
   *Integrate safety cases with mainstream documentation and update them continuously (not sideline after approval).*

---

> **Final Exam Tip**: Always connect safety engineering principles to security applications. Remember the **Satan's computer** concept as the core differentiator in security engineering challenges.
